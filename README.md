This report aims to explore the domain of machine translation, specifically the task of converting sequences from a source language (Spanish) to a target language (English), utilizing sequence-to-sequence (seq2seq) learning as defined by Chollet (2017). Our approach leverages Recurrent Neural Networks (RNNs) within the Encoder-Decoder framework, incorporating an attention mechanism and the Transformer model to enhance translation accuracy. We train our models on a dataset comprising 125,111 Spanish-English sequence pairs and validate their performance on a test set of 13,902 pairs. Throughout the training phase, we closely monitor model loss and accuracy metrics, while for evaluation on the test data, we employ the Rouge-L score and BLEU metrics to assess the quality of the translations produced.
